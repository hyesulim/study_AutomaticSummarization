{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Text with Amazon Reviews - version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Description\n",
    "1. **Project**: Amazon에서 팔린 식품의 리뷰를 요약하는 모델을 만드는 프로젝트로, [블로그](https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b)와 [Github](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews)를 보고 참고함. 공부의 목적으로 이 프로젝트를 그대로 따라해 보는 중임.<br /><br />\n",
    "2. **Data** : *Amazon Fine Food Reviews*. [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews)에서 다운로드함.<br />리뷰 내용(description)을 input으로 하고, 리뷰의 제목(title)을 target으로 하여 description이 text, title이 summary이다.\n",
    "<br /><br />\n",
    "3. **Tools** : Python, Tensorflow 1.2.1\n",
    "<br /><br />\n",
    "4. **Model**: 인코딩 레이어에 **bi-directional RNN과 LSTMs**을 사용하고, 디코딩 레이어에 **attention**을 사용한다. Textsum에서 사용한 seq2seq 모델과 유사함.\n",
    "<br /><br />\n",
    "5. **Sections** :\n",
    "    - Inspection the Data\n",
    "    - Preparing the Data\n",
    "    - Building the Model\n",
    "    - Training the Model\n",
    "    - Making Our Own Summaries\n",
    "<br/><br/>\n",
    "6. **NEW for version2** : version2 에서는 data split 을 하고, evaluation을 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/limhyesu/Summarization_study_data/Reviews.csv'\n",
    "reviews = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.read_csv returns into DataFrame <br/>\n",
    "reviews : DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any nulls values. 칼럽별 null 개수 구하기.\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values and unneeded features\n",
    "# drop null values\n",
    "reviews = reviews.dropna()\n",
    "# drop unneeded features. only Summary and Text remain.\n",
    "reviews = reviews.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', \n",
    "                        'HelpfulnessDenominator', 'Score', 'Time'], 1)\n",
    "# drop parameter avoids the old index being added as column\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Data\n",
    "- convert to lowercase\n",
    "- replace contractions with their longer forms. (contraction : 줄임말 등)\n",
    "- remove any unwanted characters (done after replacing contractions. backward slash before hyphen.)\n",
    "- remove stopwords from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# someone made contraction dictionary from wikipeda\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word]) # longer term을 nex_text에 append함.\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        # join() method takes all items in an iterable and joins them into one string.\n",
    "        text = \" \".join(new_text)        \n",
    "        \n",
    "        # Format words and remove unwanted characters\n",
    "        # ?는 0번 또는 1차례까지의 발생을 의미함. http 또는 https를 의미함.\n",
    "        # MULTILINE : '^'가 각 문자열, 문장의 처음에 매칭됨. '$'는 각 문자열과 문장의 마지막에 매칭됨.\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'\\&amp;', ' ', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'\\&amp;', ' ', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        \n",
    "        # Optionally, remove stop words\n",
    "        if remove_stopwords:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START Data Split\n",
    "Split data into two parts : Train, Test<br/>\n",
    "**Train** - text_train, summary_train<br/>\n",
    "**Test** - text_test, summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_attribute = reviews['Summary']\n",
    "tmp = reviews.drop(columns=['Summary'], axis=1)\n",
    "text_train, text_test, summary_train, summary_test = train_test_split(tmp, target_attribute, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "summary_test = pd.DataFrame(summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_train = pd.DataFrame(summary_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the summaries and texts\n",
    "# stopwords will only be removed from the description to make training faster\n",
    "# but they will reamin in the summaries to make them sound more like natural phrases.\n",
    "clean_summaries_train = []\n",
    "for summary in summary_train.Summary:\n",
    "    clean_summaries_train.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries for train are complete.\")\n",
    "\n",
    "clean_texts_train = []\n",
    "for text in text_train.Text:\n",
    "    clean_texts_train.append(clean_text(text, remove_stopwords=True))\n",
    "print(\"Texts for train are complete.\")\n",
    "\n",
    "clean_summaries_test = []\n",
    "for summary in summary_test.Summary:\n",
    "    clean_summaries_test.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries for test are complete.\")\n",
    "\n",
    "clean_texts_test = []\n",
    "for text in text_test.Text:\n",
    "    clean_texts_test.append(clean_text(text, remove_stopwords=True))\n",
    "print(\"Texts for test are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned all.\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\", i+1)\n",
    "    print(clean_summaries_train[i])\n",
    "    print(clean_texts_train[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    # build word histogram as dictioncary to count the word\n",
    "    \n",
    "    for sentence in text: # text가 하나의 문장을 element로 가진 배열\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary both in summary and text\n",
    "# Summary와 Text에 나타나는 서로 다른 단어의 종류 개수.\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries_train)\n",
    "count_words(word_counts, clean_texts_train)\n",
    "\n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better\n",
    "<br />\n",
    "- use pre-trained word vectors to help improve the performance of our model.\n",
    "- **ConceptNet Numberbatch(CN)** : word embeddings that we are using.\n",
    "- **ConceptNet** : semantic network. 컴퓨터가 자연어의 의미를 이해하는데 도움이 되도록 만들어짐. https://github.com/commonsense/conceptnet-numberbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary\n",
    "embeddings_index = {}\n",
    "\n",
    "# f로 파일을 오픈함. f의 각 line에 대하여, line을 단어 별로 나누어 values에 할당. word는 value[0].\n",
    "# float32 type으로 values[1:]를 numpy array로 convert하여 embedding에 저장.\n",
    "# word가 key, embedding이 value.\n",
    "\n",
    "with open('/home/limhyesu/Summarization_study_data/numberbatch-en-17.04b.txt', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "        \n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold\n",
    "# CN에 없고 threshold인 20번 보다 많이 등장한 단어의 개수를 찾는다. -> missing_words 에 추가\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "# Dict.items() returns dict_items object that connects key and value.\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "\n",
    "missing_ratio = round(missing_words/len(word_counts), 4)*100\n",
    "\n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CN에 없는 단어가 word_embedding_matrix에 더해지려면 그 단어는 적어도 20번 이상 등장해야 한다. 많이 등장해야 모델이 단어의 의미를 이해할 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the vocab that we will use to words that appear >= threshold or are in Glove\n",
    "\n",
    "# dictionary to convert words to integers\n",
    "vocab_to_int = {}\n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index: # 20번 이상 등장하거나, CN에 있다면\n",
    "        vocab_to_int[word] = value # 단어마다 int 할당. \n",
    "        value += 1\n",
    "        \n",
    "# summary 나 text에 등장하는 단어 중\n",
    "# vocab_to_int에는 CN에 있거나, CN에는 없지만 20번 이상 등장하는 단어가 key\n",
    "# 각 key에 대해 0부터 차례대로 int 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Total number of unique words:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcial tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<GO>\"]\n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int) # vocab_to_int의 마지막에 codes 추가.\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_ratio = round(len(vocab_to_int)/ len(word_counts), 4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- limit vocabulary to words that are either **in CN** or **occur more than 20 times** in our dataset\n",
    "- model이 단어를 많이 볼 수록, 즉 단어가 많이 나타날 수록 단어들끼리 어떻게 연관되어 있는지 알기 쉽기 때문에 어휘를 위와 같이 제한하는 것이 좋은 word embedding을 만들 수 있다.\n",
    "- word_embedding_matrix를 만들 때 np.zeros의 dtype을 float32로 설정하는 것은 매우 중요하다. 초기값이 float64인데 이는 Tensorflow에서 안돌아가므로 32로 낮춰야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# np.zeros(shape, type, order) : return a new array of given shape and type, filled with zeros.\n",
    "# (nb_words, emabedding_dim) : shape of the new array.\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype = np.float32)\n",
    "\n",
    "# vocab_to_int : 사용할 단어, embeddings_index : CN에 있는 단어\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index: \n",
    "        # CN에 있는 word라면 embedding 그대로 추가\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # if word not in CN, create a random embedding for it \n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding # embeddings_index에 word와 embedding 추가함.\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "        \n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "        If word is not in vocab_to_int, use UNK's integer.\n",
    "        Total the number of words and UNKs.\n",
    "        Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentences_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentences_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentences_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentences_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentences_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "# summary를 int로 바꿈, text를 int로 바꾸고 eos 추가.\n",
    "int_summaries_train, word_count, unk_count = convert_to_ints(clean_summaries_train, word_count, unk_count)\n",
    "int_texts_train, word_count, unk_count = convert_to_ints(clean_texts_train, word_count, unk_count, eos = True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count, 4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    # 각 sentence의 length를 counts 열에 작성.\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summaries_train = create_lengths(int_summaries_train)\n",
    "lengths_texts_train = create_lengths(int_texts_train)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries_train.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts_train.counts, 90)) # compute the 90th percentile of the lengths_texts elements\n",
    "print(np.percentile(lengths_texts_train.counts, 95))\n",
    "print(np.percentile(lengths_texts_train.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries_train.counts, 90))\n",
    "print(np.percentile(lengths_summaries_train.counts, 95))\n",
    "print(np.percentile(lengths_summaries_train.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of the UNK appears in a sentence'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To help train the model faster, **sort** the reviews by the **length of the descriptions** form shortest to longest.\n",
    "- This maeks each batch to have descriptions of **similar lengths**, which will result int **less padding**, thus **less computing**.\n",
    "- Some reviews will not be included because of the number of UNK tokens in the description or summary. If there is more than 1 UNK in the description or any UNKs in the summary, the review will not be used. 의미있는 데이터로 모델을 만들고 싶기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to largest\n",
    "# Sorting is to make each batch to have descriptions of similar lengths, which will result in less padding, thus less computing.\n",
    "# Limit the length of summaries and texts based on the min and max ranges\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84 # 90% percentile\n",
    "max_summary_length = 13 # 99% percentile\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "# text에는 1개의 UNK 까지 허용. summary에는 UNK가 없어야 함.\n",
    "\n",
    "for length in range(min(lengths_texts_train.counts), max_text_length):\n",
    "    for count, words in enumerate(int_summaries_train): # enumerate : 몇 번 째 반복문인지 확인 가능.\n",
    "        if(len(int_summaries_train[count]) >= min_length and\n",
    "           len(int_summaries_train[count]) <= max_summary_length and\n",
    "           len(int_texts_train[count]) >= min_length and\n",
    "           unk_counter(int_summaries_train[count]) <= unk_summary_limit and\n",
    "           unk_counter(int_texts_train[count]) <= unk_text_limit and\n",
    "           length == len(int_texts_train[count]) # min, max 사이의 범위에 있는 length가 text의 length일 때.\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries_train[count])\n",
    "            sorted_texts.append(int_texts_train[count])\n",
    "\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for input to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # summary_length, text_length are the lengths of each sentence within a batch\n",
    "    # max_summary_length is maximum length of a summary within a batch\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    \n",
    "    # Computes the maximum of elements across dimensions of a tensor.\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "    \n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    # ending = target_data의 마지막 단어를 추출함.\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    # dec_input = scalar value로 채워진 tensor를 만들어서 마지막에 <GO> 붙임.\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            # Cell 만들기\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                   input_keep_prob = keep_prob)\n",
    "            \n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                             initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw,\n",
    "                                                    input_keep_prob = keep_prob)\n",
    "            ### MY CODE START\n",
    "            \n",
    "            \n",
    "            # Cell 구동\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                                    cell_bw, rnn_inputs, sequence_length,\n",
    "                                                                    dtype = tf.float32)\n",
    "            \n",
    "            '''\n",
    "            ((enc_output_fw, enc_output_bw),\n",
    "            (enc_state_fw, enc_state_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, \n",
    "                                                                            rnn_inputs, sequence_length, dtype=tf.float32)\n",
    "            '''\n",
    "            # Join outputs since we are using a bidirectional RNN\n",
    "            enc_output = tf.concat(enc_output, 2)\n",
    "            '''\n",
    "            enc_output = tf.concat((enc_output_fw, enc_output_bw), 2)\n",
    "            \n",
    "            enc_state = []\n",
    "            for i in range(num_layers):\n",
    "                if isinstance(enc_state_fw[i], tf.contrib.rnn.LSTMStateTuple):\n",
    "                    enc_state_c = tf.concat(values=(enc_state_fw[i].c, enc_state_bw[i].c), \n",
    "                                            axis=1, name=\"enc_state_fw_c\")\n",
    "                    enc_state_h = tf.concat(values=(enc_state_fw[i].h, enc_state_bw[i].h), \n",
    "                                            axis=1, name=\"enc_state_fw_h\")\n",
    "                    enc_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=enc_state_h)\n",
    "                elif isinstance(enc_state_fw[i], tf.Tensor):\n",
    "                    enc_state = tf.concat(values=(enc_state_fw[i], enc_state_bw[i]), \n",
    "                                          axis=1, name='bidirectional_concat')\n",
    "            \n",
    "            enc_state = tuple(enc_state)\n",
    "            '''\n",
    "            \n",
    "            return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer,\n",
    "                           vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                       sequence_length=summary_length,\n",
    "                                                       time_major=False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                      training_helper,\n",
    "                                                      initial_state,\n",
    "                                                      output_layer)\n",
    "    training_logits, *_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                          output_time_major=False,\n",
    "                                                          impute_finished=True,\n",
    "                                                          maximum_iterations=max_summary_length)\n",
    "        \n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TrainingHelper** reads a sequence of integers from the encoding layer.\n",
    "<br />\n",
    "**BasicDecoder** processes the sequence with the decoding cell, an output layer, which is a fully connected layer. *initial_state* comes from our *DynamicAttentionWrapperState* that you will see soon.\n",
    "<br />\n",
    "**dynamic_decode** creates our outputs that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    ### MY CODE START\n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype = tf.int32), [batch_size], name='start_tokens')\n",
    "    #start_toekns = tf.contrib.seq2seq.tile_batch(tf.constant([start_token], dtype = tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                               start_tokens, end_token)\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state,\n",
    "                                                       output_layer)\n",
    "    \n",
    "    inference_logits, *_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**inference_decoding_layer** is very similar to training layer. The main difference is **GreedyEmbeddingHelper**, which uses the argmax of the output (treated as logits) and passes the result through an embedding layer to get the next input. Although it is asking for **start_tokens**, we only have one, < GO >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                  max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the trainig and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                          initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "            \n",
    "    output_layer = Dense(vocab_size, \n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev = 0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, text_length, normalize = False, name='BahdanauAttention')\n",
    "    \n",
    "    ### MY CODE START\n",
    "    '''No DynamicAttentionWrapper in new version so I changed the code'''\n",
    "    \n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state,\n",
    "                                                output_layer, vocab_size, max_summary_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'],\n",
    "                                                 vocab_to_int['<EOS>'],\n",
    "                                                 dec_cell, initial_state, output_layer, max_summary_length,\n",
    "                                                 batch_size)\n",
    "        \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length,\n",
    "                 vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(dec_embed_input, embeddings, enc_output, \n",
    "                                                      enc_state, vocab_size, text_length, summary_length,\n",
    "                                                      max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size,\n",
    "                                                      num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    '''Pad sentences with <PAD> so that each sentence of batch has the same length'''\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    '''Batch summaries, texts, and the lengths of their sentences together'''\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i+batch_size]\n",
    "        texts_batch = texts[start_i:start_i+batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "            \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "            \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs\n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                     targets, keep_prob, text_length, summary_length,\n",
    "                                                     max_summary_length, len(vocab_to_int)+1, rnn_size, num_layers,\n",
    "                                                     vocab_to_int, batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype = tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "print(\"Graph is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "used subset of the data since the whole data will take too long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for training\n",
    "start = 200000\n",
    "end = start + 50000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\", len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0\n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0\n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(\"./\"+checkpoint+'.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost], \n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if (batch_i % display_step == 0) and (batch_i > 0):\n",
    "                print('Epoch {:>3}/{} Batch{:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                     .format(epoch_i, epochs, batch_i, len(sorted_texts_short) // batch_size,\n",
    "                            batch_loss / display_step, batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "            if (batch_i % update_check == 0) and (batch_i > 0):\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!')\n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, checkpoint)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "                \n",
    "            \n",
    "            # Reduce learnig rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            \n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Making Our Own Summaries\n",
    "\n",
    "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Our Own Summaries\n",
    "input_sentence = clean_texts_test\n",
    "text = []\n",
    "texts_batch_words = []\n",
    "answer_logits_words = []\n",
    "\n",
    "for text_single in clean_texts_test:\n",
    "    text.append(text_to_seq(text_single))\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    pad = vocab_to_int[\"<PAD>\"]\n",
    "\n",
    "    # Multiply by batch_size to match the model's input parameters.\n",
    "    for batch_i, (_, texts_batch, _, texts_length) in enumerate(\n",
    "        get_batches(text, text, batch_size)):\n",
    "        answer_logits = sess.run(logits, {input_data: texts_batch,\n",
    "                                summary_length:[np.random.randint(5,8)],\n",
    "                                text_length: texts_length,\n",
    "                                keep_prob: 1.0})\n",
    "        \n",
    "        for j, text_i in enumerate(texts_batch):\n",
    "            texts_batch_words.append(\" \".join([int_to_vocab[i] for i in text_i if i != pad]))\n",
    "\n",
    "        for j, answer_i in enumerate(answer_logits):\n",
    "            answer_logits_words.append(\" \".join([int_to_vocab[i] for i in answer_i if i != pad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = np.asarray(texts_batch_words)\n",
    "answer_summary = np.asarray(answer_logits_words)\n",
    "\n",
    "original_text = pd.DataFrame(original_text)\n",
    "answer_summary = pd.DataFrame(answer_summary)\n",
    "\n",
    "original_text.columns=[\"text\"]\n",
    "answer_summary.columns=[\"system summary\"]\n",
    "\n",
    "model_summary=pd.DataFrame({'model summary':clean_summaries_test})\n",
    "\n",
    "text_and_summary = pd.concat([original_text, answer_summary, model_summary], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_and_summary.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_file(file_name, list_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for sentence in list_name:\n",
    "            f.write(\"%s\\n\" % sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_file('model_sum.txt', text_and_summary['model summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_file('system_sum.txt', text_and_summary['system summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for sentence in text_and_summary['model summary']:\n",
    "    with open(\"./models/model_sum.{}.txt\".format(n), 'w') as f:\n",
    "        f.write(\"%s\\n\" %sentence)\n",
    "    n+=1\n",
    "    \n",
    "n=0\n",
    "for sentence in text_and_summary['system summary']:\n",
    "    with open(\"./systems/system_sum.{}.txt\".format(n), 'w') as f:\n",
    "        f.write(\"%s\\n\" %sentence)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrouge import Rouge155\n",
    "\n",
    "Rouge155.convert_summaries_to_rouge_format('./models', './models_out')\n",
    "Rouge155.convert_summaries_to_rouge_format('./systems', './systems_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyrouge import Rouge155\n",
    "'''\n",
    "Rouge155.write_config_static(\n",
    "    './systems_out', 'system_sum.(\\d+).txt',\n",
    "    './models_out', 'model_sum.(\\d+).txt',\n",
    "    './config')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge('./system_sum.txt', './model_sum.txt')\n",
    "scores = files_rouge.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드디어 rouge 스코어를 구했다..!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores 에 id column 을 추가해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rouge_score = np.asarray(scores)\n",
    "rouge_score = pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rouge_score dataframe 을 csv 파일로 변환해서 저장해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score.to_csv('rouge_score.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_file('rouge_score.txt', rouge_score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
