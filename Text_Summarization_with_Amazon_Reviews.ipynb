{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Text with Amazon Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Description\n",
    "1. **Project**: Amazon에서 팔린 식품의 리뷰를 요약하는 모델을 만드는 프로젝트로, [블로그](https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b)와 [Github](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews)를 보고 참고함. 공부의 목적으로 이 프로젝트를 그대로 따라해 보는 중임.<br /><br />\n",
    "2. **Data** : *Amazon Fine Food Reviews*. [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews)에서 다운로드함.<br />리뷰 내용(description)을 input으로 하고, 리뷰의 제목(title)을 target으로 하여 description이 text, title이 summary이다.\n",
    "<br /><br />\n",
    "3. **Tools** : Python, Tensorflow 1.2.1\n",
    "<br /><br />\n",
    "4. **Model**: 인코딩 레이어에 **bi-directional RNN과 LSTMs**을 사용하고, 디코딩 레이어에 **attention**을 사용한다. Textsum에서 사용한 seq2seq 모델과 유사함.\n",
    "<br /><br />\n",
    "5. **Sections** :\n",
    "    - Inspection the Data\n",
    "    - Preparing the Data\n",
    "    - Building the Model\n",
    "    - Training the Model\n",
    "    - Making Our Own Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/home/limhyesu/Summarization_study_data/Reviews.csv'\n",
    "reviews = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>B0001PB9FE</td>\n",
       "      <td>A3HDKO7OW0QNK4</td>\n",
       "      <td>Canadian Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1107820800</td>\n",
       "      <td>The Best Hot Sauce in the World</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>B0009XLVG0</td>\n",
       "      <td>A2725IB4YY9JEB</td>\n",
       "      <td>A Poeng \"SparkyGoHome\"</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1282867200</td>\n",
       "      <td>My cats LOVE this \"diet\" food better than thei...</td>\n",
       "      <td>One of my boys needed to lose some weight and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>B0009XLVG0</td>\n",
       "      <td>A327PCT23YH90</td>\n",
       "      <td>LT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1339545600</td>\n",
       "      <td>My Cats Are Not Fans of the New Food</td>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A18ECVX2RJ7HUE</td>\n",
       "      <td>willie \"roadie\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1288915200</td>\n",
       "      <td>fresh and greasy!</td>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A2MUGFV2TDQ47K</td>\n",
       "      <td>Lynrie \"Oh HELL no\"</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1268352000</td>\n",
       "      <td>Strawberry Twizzlers - Yummy</td>\n",
       "      <td>The Strawberry Twizzlers are my guilty pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A1CZX3CP8IKQIJ</td>\n",
       "      <td>Brian A. Lee</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1262044800</td>\n",
       "      <td>Lots of twizzlers, just what you expect.</td>\n",
       "      <td>My daughter loves twizzlers and this shipment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A3KLWF6WQ5BNYO</td>\n",
       "      <td>Erica Neathery</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1348099200</td>\n",
       "      <td>poor taste</td>\n",
       "      <td>I love eating them and they are good for watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>AFKW14U97Z6QO</td>\n",
       "      <td>Becca</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1345075200</td>\n",
       "      <td>Love it!</td>\n",
       "      <td>I am very satisfied with my Twizzler purchase....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A2A9X58G2GTBLP</td>\n",
       "      <td>Wolfee1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1324598400</td>\n",
       "      <td>GREAT SWEET CANDY!</td>\n",
       "      <td>Twizzlers, Strawberry my childhood favorite ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A3IV7CL2C13K2U</td>\n",
       "      <td>Greg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1318032000</td>\n",
       "      <td>Home delivered twizlers</td>\n",
       "      <td>Candy was delivered very fast and was purchase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A1WO0KGLPR5PV6</td>\n",
       "      <td>mom2emma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1313452800</td>\n",
       "      <td>Always fresh</td>\n",
       "      <td>My husband is a Twizzlers addict.  We've bough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>AZOF9E17RGZH8</td>\n",
       "      <td>Tammy Anderson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1308960000</td>\n",
       "      <td>TWIZZLERS</td>\n",
       "      <td>I bought these for my husband who is currently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>ARYVQL4N737A1</td>\n",
       "      <td>Charles Brown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1304899200</td>\n",
       "      <td>Delicious product!</td>\n",
       "      <td>I can remember buying this candy as a kid and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>AJ613OLZZUG7V</td>\n",
       "      <td>Mare's</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1304467200</td>\n",
       "      <td>Twizzlers</td>\n",
       "      <td>I love this candy.  After weight watchers I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A22P2J09NJ9HKE</td>\n",
       "      <td>S. Cabanaugh \"jilly pepper\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1295481600</td>\n",
       "      <td>Please sell these in Mexico!!</td>\n",
       "      <td>I have lived out of the US for over 7 yrs now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A3FONPR03H3PJS</td>\n",
       "      <td>Deborah S. Linzer \"Cat Lady\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1288310400</td>\n",
       "      <td>Twizzlers - Strawberry</td>\n",
       "      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A3RXAU2N8KV45G</td>\n",
       "      <td>lady21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1332633600</td>\n",
       "      <td>Nasty No flavor</td>\n",
       "      <td>The candy is just red , No flavor . Just  plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>AAAS38B98HMIK</td>\n",
       "      <td>Heather Dube</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1331856000</td>\n",
       "      <td>Great Bargain for the Price</td>\n",
       "      <td>I was so glad Amazon carried these batteries. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>B00144C10S</td>\n",
       "      <td>A2F4LZVGFLD1OB</td>\n",
       "      <td>DaisyH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338854400</td>\n",
       "      <td>YUMMY!</td>\n",
       "      <td>I got this for my Mum who is not diabetic but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>B0001PB9FY</td>\n",
       "      <td>A3HDKO7OW0QNK4</td>\n",
       "      <td>Canadian Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1107820800</td>\n",
       "      <td>The Best Hot Sauce in the World</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568424</th>\n",
       "      <td>568425</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A291HTT117RVS9</td>\n",
       "      <td>MadisonVeggie \"Lynette in Wisconsin\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1333411200</td>\n",
       "      <td>Unusual, even for a violet candy</td>\n",
       "      <td>I've tried several violet flavored candies in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568425</th>\n",
       "      <td>568426</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A1TRJQBYQFZW4</td>\n",
       "      <td>Brad</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1325376000</td>\n",
       "      <td>Very good</td>\n",
       "      <td>This candy has a very good flavor. It is quite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568426</th>\n",
       "      <td>568427</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A2F25C6QCKJ3HL</td>\n",
       "      <td>Jill</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1341532800</td>\n",
       "      <td>rip off</td>\n",
       "      <td>The candy is tasty, but they totally scam you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568427</th>\n",
       "      <td>568428</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A7VLVCQJIROTF</td>\n",
       "      <td>Lorraine \"DamnSwank\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1340323200</td>\n",
       "      <td>The search has ended!</td>\n",
       "      <td>I had been looking for the violet candy with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568428</th>\n",
       "      <td>568429</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A1YGG6T4YJZWC1</td>\n",
       "      <td>bethany \"birds and more birds\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1340236800</td>\n",
       "      <td>very tasty</td>\n",
       "      <td>these are very pricey so i only enjoy them now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568429</th>\n",
       "      <td>568430</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>AW7HIV8I53FE4</td>\n",
       "      <td>Herb Schulsinger \"Bach Because!\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1338508800</td>\n",
       "      <td>Violet French Hard Candy</td>\n",
       "      <td>These candies have a mild flavor, when compare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568430</th>\n",
       "      <td>568431</td>\n",
       "      <td>B001FPT1WM</td>\n",
       "      <td>A1XDMZMMOAMR7</td>\n",
       "      <td>nyxport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1345161600</td>\n",
       "      <td>delicious</td>\n",
       "      <td>This product is a bit pricey for the amt. rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568431</th>\n",
       "      <td>568432</td>\n",
       "      <td>B003XUL27E</td>\n",
       "      <td>A32Y0419QFGVHM</td>\n",
       "      <td>Kevin Mitchell</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1319673600</td>\n",
       "      <td>Mostly water</td>\n",
       "      <td>Definitely not worth buying flavored water wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568432</th>\n",
       "      <td>568433</td>\n",
       "      <td>B003XUL27E</td>\n",
       "      <td>A1YKXVGAIA8952</td>\n",
       "      <td>NC65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1319500800</td>\n",
       "      <td>No Good</td>\n",
       "      <td>I thought this soup would be more like a chill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568433</th>\n",
       "      <td>568434</td>\n",
       "      <td>B003XUL27E</td>\n",
       "      <td>A1JUG9WCN1A52Z</td>\n",
       "      <td>maudlin666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1311638400</td>\n",
       "      <td>Tastes horrible!</td>\n",
       "      <td>I just bought this soup today at my local groc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568434</th>\n",
       "      <td>568435</td>\n",
       "      <td>B003XUL27E</td>\n",
       "      <td>ABGQPE97ZVYJ3</td>\n",
       "      <td>Katherine Kelly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>Not so good</td>\n",
       "      <td>This soup is mostly broth. Although it has a k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568435</th>\n",
       "      <td>568436</td>\n",
       "      <td>B003XUL27E</td>\n",
       "      <td>A2PSB4WQHH46HN</td>\n",
       "      <td>carlyowu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1285718400</td>\n",
       "      <td>Where's the tortellini?</td>\n",
       "      <td>It is mostly broth, with the advertised 3/4 cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568436</th>\n",
       "      <td>568437</td>\n",
       "      <td>B000NY4SAG</td>\n",
       "      <td>A3ODWHC3EMMWTT</td>\n",
       "      <td>K. Brennan \"Baking fool\"</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1229731200</td>\n",
       "      <td>Baker's Ammonia Small quantity</td>\n",
       "      <td>In the past, I would have to buy a large quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568437</th>\n",
       "      <td>568438</td>\n",
       "      <td>B000NY4SAG</td>\n",
       "      <td>A1RKKPSXF9QIZF</td>\n",
       "      <td>Allen \"RcDriver\"</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1233792000</td>\n",
       "      <td>Good buy.</td>\n",
       "      <td>Ammonium bicarbonate in a nice little package....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568438</th>\n",
       "      <td>568439</td>\n",
       "      <td>B000NY4SAG</td>\n",
       "      <td>A3M89SF0SSOGBK</td>\n",
       "      <td>Robert Goode \"GrampaG\"</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1316995200</td>\n",
       "      <td>Baking Ammonia that Works</td>\n",
       "      <td>If you haven't ever used Ammonium Bicarbonate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568439</th>\n",
       "      <td>568440</td>\n",
       "      <td>B000NY4SAG</td>\n",
       "      <td>A34GDV49ZZQKXG</td>\n",
       "      <td>Donald M. Cook</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1280102400</td>\n",
       "      <td>a-ok</td>\n",
       "      <td>We need this for a recipe my wife is intereste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568440</th>\n",
       "      <td>568441</td>\n",
       "      <td>B005ZC0RRO</td>\n",
       "      <td>A2TO5R8QLIITEF</td>\n",
       "      <td>SAK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1323734400</td>\n",
       "      <td>Delicious, all natural and allergy free treats!</td>\n",
       "      <td>Indie Candy's gummies are absolutely delicious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568441</th>\n",
       "      <td>568442</td>\n",
       "      <td>B000NY8O9M</td>\n",
       "      <td>AZRHU8CP5XKMF</td>\n",
       "      <td>David L. Brown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1264204800</td>\n",
       "      <td>Great For Fast Gulasch!</td>\n",
       "      <td>Quick and easy! Had similar Gulasch in Guest H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568442</th>\n",
       "      <td>568443</td>\n",
       "      <td>B006T7TKZO</td>\n",
       "      <td>A3BOURUK79CYY5</td>\n",
       "      <td>BIH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338854400</td>\n",
       "      <td>Great Cafe Latte</td>\n",
       "      <td>This product is great.  Gives you so much ener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568443</th>\n",
       "      <td>568444</td>\n",
       "      <td>B000H7K114</td>\n",
       "      <td>A2AGSSZR9V7XST</td>\n",
       "      <td>Peter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1281744000</td>\n",
       "      <td>Excellent Tea</td>\n",
       "      <td>I love this tea.  I first discovered the pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568444</th>\n",
       "      <td>568445</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A2SD7TY3IOX69B</td>\n",
       "      <td>BayBay \"BayBay Knows Best\"</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1245369600</td>\n",
       "      <td>Best Value for Chinese 5 Spice</td>\n",
       "      <td>As a foodie, I use a lot of Chinese 5 Spice po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568445</th>\n",
       "      <td>568446</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A2E5C8TTAED4CQ</td>\n",
       "      <td>S. Linkletter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1268006400</td>\n",
       "      <td>Five Spice Powder</td>\n",
       "      <td>You can make this mix yourself, but the Star A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568446</th>\n",
       "      <td>568447</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A2P9W8T7NTLG2Z</td>\n",
       "      <td>Andy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1328918400</td>\n",
       "      <td>Mixed wrong</td>\n",
       "      <td>I had ordered some of these a few months back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568447</th>\n",
       "      <td>568448</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>APWCOAVILK94B</td>\n",
       "      <td>Real Named Person \"wowzee\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1322524800</td>\n",
       "      <td>If its all natural, this is like panacea of Sp...</td>\n",
       "      <td>Hoping there is no MSG in this, this tastes ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568448</th>\n",
       "      <td>568449</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A1F6BHEYB7R6R7</td>\n",
       "      <td>James Braley</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1308096000</td>\n",
       "      <td>Very large ground spice jars.</td>\n",
       "      <td>My only complaint is that there's so much of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T   \n",
       "5            6  B006K2ZZ7K   ADT0SRK1MGOEU   \n",
       "6            7  B006K2ZZ7K  A1SP2KVKFXXRU1   \n",
       "7            8  B006K2ZZ7K  A3JRGQVEQN31IQ   \n",
       "8            9  B000E7L2R4  A1MZYO9TZK0BBI   \n",
       "9           10  B00171APVA  A21BT40VZCCYT4   \n",
       "10          11  B0001PB9FE  A3HDKO7OW0QNK4   \n",
       "11          12  B0009XLVG0  A2725IB4YY9JEB   \n",
       "12          13  B0009XLVG0   A327PCT23YH90   \n",
       "13          14  B001GVISJM  A18ECVX2RJ7HUE   \n",
       "14          15  B001GVISJM  A2MUGFV2TDQ47K   \n",
       "15          16  B001GVISJM  A1CZX3CP8IKQIJ   \n",
       "16          17  B001GVISJM  A3KLWF6WQ5BNYO   \n",
       "17          18  B001GVISJM   AFKW14U97Z6QO   \n",
       "18          19  B001GVISJM  A2A9X58G2GTBLP   \n",
       "19          20  B001GVISJM  A3IV7CL2C13K2U   \n",
       "20          21  B001GVISJM  A1WO0KGLPR5PV6   \n",
       "21          22  B001GVISJM   AZOF9E17RGZH8   \n",
       "22          23  B001GVISJM   ARYVQL4N737A1   \n",
       "23          24  B001GVISJM   AJ613OLZZUG7V   \n",
       "24          25  B001GVISJM  A22P2J09NJ9HKE   \n",
       "25          26  B001GVISJM  A3FONPR03H3PJS   \n",
       "26          27  B001GVISJM  A3RXAU2N8KV45G   \n",
       "27          28  B001GVISJM   AAAS38B98HMIK   \n",
       "28          29  B00144C10S  A2F4LZVGFLD1OB   \n",
       "29          30  B0001PB9FY  A3HDKO7OW0QNK4   \n",
       "...        ...         ...             ...   \n",
       "568424  568425  B001FPT1WM  A291HTT117RVS9   \n",
       "568425  568426  B001FPT1WM   A1TRJQBYQFZW4   \n",
       "568426  568427  B001FPT1WM  A2F25C6QCKJ3HL   \n",
       "568427  568428  B001FPT1WM   A7VLVCQJIROTF   \n",
       "568428  568429  B001FPT1WM  A1YGG6T4YJZWC1   \n",
       "568429  568430  B001FPT1WM   AW7HIV8I53FE4   \n",
       "568430  568431  B001FPT1WM   A1XDMZMMOAMR7   \n",
       "568431  568432  B003XUL27E  A32Y0419QFGVHM   \n",
       "568432  568433  B003XUL27E  A1YKXVGAIA8952   \n",
       "568433  568434  B003XUL27E  A1JUG9WCN1A52Z   \n",
       "568434  568435  B003XUL27E   ABGQPE97ZVYJ3   \n",
       "568435  568436  B003XUL27E  A2PSB4WQHH46HN   \n",
       "568436  568437  B000NY4SAG  A3ODWHC3EMMWTT   \n",
       "568437  568438  B000NY4SAG  A1RKKPSXF9QIZF   \n",
       "568438  568439  B000NY4SAG  A3M89SF0SSOGBK   \n",
       "568439  568440  B000NY4SAG  A34GDV49ZZQKXG   \n",
       "568440  568441  B005ZC0RRO  A2TO5R8QLIITEF   \n",
       "568441  568442  B000NY8O9M   AZRHU8CP5XKMF   \n",
       "568442  568443  B006T7TKZO  A3BOURUK79CYY5   \n",
       "568443  568444  B000H7K114  A2AGSSZR9V7XST   \n",
       "568444  568445  B001EO7N10  A2SD7TY3IOX69B   \n",
       "568445  568446  B001EO7N10  A2E5C8TTAED4CQ   \n",
       "568446  568447  B001EO7N10  A2P9W8T7NTLG2Z   \n",
       "568447  568448  B001EO7N10   APWCOAVILK94B   \n",
       "568448  568449  B001EO7N10  A1F6BHEYB7R6R7   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC   \n",
       "\n",
       "                                 ProfileName  HelpfulnessNumerator  \\\n",
       "0                                 delmartian                     1   \n",
       "1                                     dll pa                     0   \n",
       "2            Natalia Corres \"Natalia Corres\"                     1   \n",
       "3                                       Karl                     3   \n",
       "4              Michael D. Bigham \"M. Wassir\"                     0   \n",
       "5                             Twoapennything                     0   \n",
       "6                          David C. Sullivan                     0   \n",
       "7                         Pamela G. Williams                     0   \n",
       "8                                   R. James                     1   \n",
       "9                              Carol A. Reed                     0   \n",
       "10                              Canadian Fan                     1   \n",
       "11                    A Poeng \"SparkyGoHome\"                     4   \n",
       "12                                        LT                     1   \n",
       "13                           willie \"roadie\"                     2   \n",
       "14                       Lynrie \"Oh HELL no\"                     4   \n",
       "15                              Brian A. Lee                     4   \n",
       "16                            Erica Neathery                     0   \n",
       "17                                     Becca                     0   \n",
       "18                                   Wolfee1                     0   \n",
       "19                                      Greg                     0   \n",
       "20                                  mom2emma                     0   \n",
       "21                            Tammy Anderson                     0   \n",
       "22                             Charles Brown                     0   \n",
       "23                                    Mare's                     0   \n",
       "24               S. Cabanaugh \"jilly pepper\"                     0   \n",
       "25              Deborah S. Linzer \"Cat Lady\"                     0   \n",
       "26                                    lady21                     0   \n",
       "27                              Heather Dube                     0   \n",
       "28                                    DaisyH                     0   \n",
       "29                              Canadian Fan                     1   \n",
       "...                                      ...                   ...   \n",
       "568424  MadisonVeggie \"Lynette in Wisconsin\"                     2   \n",
       "568425                                  Brad                     2   \n",
       "568426                                  Jill                     1   \n",
       "568427                  Lorraine \"DamnSwank\"                     1   \n",
       "568428        bethany \"birds and more birds\"                     1   \n",
       "568429      Herb Schulsinger \"Bach Because!\"                     1   \n",
       "568430                               nyxport                     0   \n",
       "568431                        Kevin Mitchell                     0   \n",
       "568432                                  NC65                     0   \n",
       "568433                            maudlin666                     0   \n",
       "568434                       Katherine Kelly                     0   \n",
       "568435                              carlyowu                     0   \n",
       "568436              K. Brennan \"Baking fool\"                    10   \n",
       "568437                      Allen \"RcDriver\"                     4   \n",
       "568438                Robert Goode \"GrampaG\"                     3   \n",
       "568439                        Donald M. Cook                     0   \n",
       "568440                                   SAK                     1   \n",
       "568441                        David L. Brown                     0   \n",
       "568442                                   BIH                     0   \n",
       "568443                                 Peter                     0   \n",
       "568444            BayBay \"BayBay Knows Best\"                     3   \n",
       "568445                         S. Linkletter                     2   \n",
       "568446                                  Andy                     0   \n",
       "568447            Real Named Person \"wowzee\"                     0   \n",
       "568448                          James Braley                     0   \n",
       "568449                      Lettie D. Carter                     0   \n",
       "568450                             R. Sawyer                     0   \n",
       "568451                         pksd \"pk_007\"                     2   \n",
       "568452               Kathy A. Welch \"katwel\"                     1   \n",
       "568453                              srfell17                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "0                            1      5  1303862400   \n",
       "1                            0      1  1346976000   \n",
       "2                            1      4  1219017600   \n",
       "3                            3      2  1307923200   \n",
       "4                            0      5  1350777600   \n",
       "5                            0      4  1342051200   \n",
       "6                            0      5  1340150400   \n",
       "7                            0      5  1336003200   \n",
       "8                            1      5  1322006400   \n",
       "9                            0      5  1351209600   \n",
       "10                           1      5  1107820800   \n",
       "11                           4      5  1282867200   \n",
       "12                           1      1  1339545600   \n",
       "13                           2      4  1288915200   \n",
       "14                           5      5  1268352000   \n",
       "15                           5      5  1262044800   \n",
       "16                           0      2  1348099200   \n",
       "17                           0      5  1345075200   \n",
       "18                           0      5  1324598400   \n",
       "19                           0      5  1318032000   \n",
       "20                           0      5  1313452800   \n",
       "21                           0      5  1308960000   \n",
       "22                           0      5  1304899200   \n",
       "23                           0      5  1304467200   \n",
       "24                           0      5  1295481600   \n",
       "25                           0      5  1288310400   \n",
       "26                           1      1  1332633600   \n",
       "27                           1      4  1331856000   \n",
       "28                           0      5  1338854400   \n",
       "29                           1      5  1107820800   \n",
       "...                        ...    ...         ...   \n",
       "568424                       2      5  1333411200   \n",
       "568425                       2      5  1325376000   \n",
       "568426                       1      1  1341532800   \n",
       "568427                       1      5  1340323200   \n",
       "568428                       1      5  1340236800   \n",
       "568429                       1      5  1338508800   \n",
       "568430                       0      5  1345161600   \n",
       "568431                       0      1  1319673600   \n",
       "568432                       0      1  1319500800   \n",
       "568433                       0      1  1311638400   \n",
       "568434                       0      2  1306368000   \n",
       "568435                       2      2  1285718400   \n",
       "568436                      10      5  1229731200   \n",
       "568437                       4      5  1233792000   \n",
       "568438                       3      5  1316995200   \n",
       "568439                       5      4  1280102400   \n",
       "568440                       1      5  1323734400   \n",
       "568441                       0      5  1264204800   \n",
       "568442                       0      5  1338854400   \n",
       "568443                       0      5  1281744000   \n",
       "568444                       3      5  1245369600   \n",
       "568445                       2      5  1268006400   \n",
       "568446                       0      2  1328918400   \n",
       "568447                       0      5  1322524800   \n",
       "568448                       0      5  1308096000   \n",
       "568449                       0      5  1299628800   \n",
       "568450                       0      2  1331251200   \n",
       "568451                       2      5  1329782400   \n",
       "568452                       1      5  1331596800   \n",
       "568453                       0      5  1338422400   \n",
       "\n",
       "                                                  Summary  \\\n",
       "0                                   Good Quality Dog Food   \n",
       "1                                       Not as Advertised   \n",
       "2                                   \"Delight\" says it all   \n",
       "3                                          Cough Medicine   \n",
       "4                                             Great taffy   \n",
       "5                                              Nice Taffy   \n",
       "6           Great!  Just as good as the expensive brands!   \n",
       "7                                  Wonderful, tasty taffy   \n",
       "8                                              Yay Barley   \n",
       "9                                        Healthy Dog Food   \n",
       "10                        The Best Hot Sauce in the World   \n",
       "11      My cats LOVE this \"diet\" food better than thei...   \n",
       "12                   My Cats Are Not Fans of the New Food   \n",
       "13                                      fresh and greasy!   \n",
       "14                           Strawberry Twizzlers - Yummy   \n",
       "15               Lots of twizzlers, just what you expect.   \n",
       "16                                             poor taste   \n",
       "17                                               Love it!   \n",
       "18                                     GREAT SWEET CANDY!   \n",
       "19                                Home delivered twizlers   \n",
       "20                                           Always fresh   \n",
       "21                                              TWIZZLERS   \n",
       "22                                     Delicious product!   \n",
       "23                                              Twizzlers   \n",
       "24                          Please sell these in Mexico!!   \n",
       "25                                 Twizzlers - Strawberry   \n",
       "26                                        Nasty No flavor   \n",
       "27                            Great Bargain for the Price   \n",
       "28                                                 YUMMY!   \n",
       "29                        The Best Hot Sauce in the World   \n",
       "...                                                   ...   \n",
       "568424                   Unusual, even for a violet candy   \n",
       "568425                                          Very good   \n",
       "568426                                            rip off   \n",
       "568427                              The search has ended!   \n",
       "568428                                         very tasty   \n",
       "568429                           Violet French Hard Candy   \n",
       "568430                                          delicious   \n",
       "568431                                       Mostly water   \n",
       "568432                                            No Good   \n",
       "568433                                   Tastes horrible!   \n",
       "568434                                        Not so good   \n",
       "568435                            Where's the tortellini?   \n",
       "568436                     Baker's Ammonia Small quantity   \n",
       "568437                                          Good buy.   \n",
       "568438                          Baking Ammonia that Works   \n",
       "568439                                               a-ok   \n",
       "568440    Delicious, all natural and allergy free treats!   \n",
       "568441                            Great For Fast Gulasch!   \n",
       "568442                                   Great Cafe Latte   \n",
       "568443                                      Excellent Tea   \n",
       "568444                     Best Value for Chinese 5 Spice   \n",
       "568445                                  Five Spice Powder   \n",
       "568446                                        Mixed wrong   \n",
       "568447  If its all natural, this is like panacea of Sp...   \n",
       "568448                      Very large ground spice jars.   \n",
       "568449                                Will not do without   \n",
       "568450                                       disappointed   \n",
       "568451                           Perfect for our maltipoo   \n",
       "568452                 Favorite Training and reward treat   \n",
       "568453                                        Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "5       I got a wild hair for taffy and ordered this f...  \n",
       "6       This saltwater taffy had great flavors and was...  \n",
       "7       This taffy is so good.  It is very soft and ch...  \n",
       "8       Right now I'm mostly just sprouting this so my...  \n",
       "9       This is a very healthy dog food. Good for thei...  \n",
       "10      I don't know if it's the cactus or the tequila...  \n",
       "11      One of my boys needed to lose some weight and ...  \n",
       "12      My cats have been happily eating Felidae Plati...  \n",
       "13      good flavor! these came securely packed... the...  \n",
       "14      The Strawberry Twizzlers are my guilty pleasur...  \n",
       "15      My daughter loves twizzlers and this shipment ...  \n",
       "16      I love eating them and they are good for watch...  \n",
       "17      I am very satisfied with my Twizzler purchase....  \n",
       "18      Twizzlers, Strawberry my childhood favorite ca...  \n",
       "19      Candy was delivered very fast and was purchase...  \n",
       "20      My husband is a Twizzlers addict.  We've bough...  \n",
       "21      I bought these for my husband who is currently...  \n",
       "22      I can remember buying this candy as a kid and ...  \n",
       "23      I love this candy.  After weight watchers I ha...  \n",
       "24      I have lived out of the US for over 7 yrs now,...  \n",
       "25      Product received is as advertised.<br /><br />...  \n",
       "26      The candy is just red , No flavor . Just  plan...  \n",
       "27      I was so glad Amazon carried these batteries. ...  \n",
       "28      I got this for my Mum who is not diabetic but ...  \n",
       "29      I don't know if it's the cactus or the tequila...  \n",
       "...                                                   ...  \n",
       "568424  I've tried several violet flavored candies in ...  \n",
       "568425  This candy has a very good flavor. It is quite...  \n",
       "568426  The candy is tasty, but they totally scam you ...  \n",
       "568427  I had been looking for the violet candy with t...  \n",
       "568428  these are very pricey so i only enjoy them now...  \n",
       "568429  These candies have a mild flavor, when compare...  \n",
       "568430  This product is a bit pricey for the amt. rece...  \n",
       "568431  Definitely not worth buying flavored water wit...  \n",
       "568432  I thought this soup would be more like a chill...  \n",
       "568433  I just bought this soup today at my local groc...  \n",
       "568434  This soup is mostly broth. Although it has a k...  \n",
       "568435  It is mostly broth, with the advertised 3/4 cu...  \n",
       "568436  In the past, I would have to buy a large quant...  \n",
       "568437  Ammonium bicarbonate in a nice little package....  \n",
       "568438  If you haven't ever used Ammonium Bicarbonate ...  \n",
       "568439  We need this for a recipe my wife is intereste...  \n",
       "568440  Indie Candy's gummies are absolutely delicious...  \n",
       "568441  Quick and easy! Had similar Gulasch in Guest H...  \n",
       "568442  This product is great.  Gives you so much ener...  \n",
       "568443  I love this tea.  I first discovered the pleas...  \n",
       "568444  As a foodie, I use a lot of Chinese 5 Spice po...  \n",
       "568445  You can make this mix yourself, but the Star A...  \n",
       "568446  I had ordered some of these a few months back ...  \n",
       "568447  Hoping there is no MSG in this, this tastes ex...  \n",
       "568448  My only complaint is that there's so much of i...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any nulls values. 칼럽별 null 개수 구하기.\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove null values and unneeded features\n",
    "# drop null values\n",
    "reviews = reviews.dropna()\n",
    "# drop unneeded features. only Summary and Text remain.\n",
    "reviews = reviews.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', \n",
    "                        'HelpfulnessDenominator', 'Score', 'Time'], 1)\n",
    "# drop parameter avoids the old index being added as column\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Best Hot Sauce in the World</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>My cats LOVE this \"diet\" food better than thei...</td>\n",
       "      <td>One of my boys needed to lose some weight and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Cats Are Not Fans of the New Food</td>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fresh and greasy!</td>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Strawberry Twizzlers - Yummy</td>\n",
       "      <td>The Strawberry Twizzlers are my guilty pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lots of twizzlers, just what you expect.</td>\n",
       "      <td>My daughter loves twizzlers and this shipment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>poor taste</td>\n",
       "      <td>I love eating them and they are good for watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Love it!</td>\n",
       "      <td>I am very satisfied with my Twizzler purchase....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GREAT SWEET CANDY!</td>\n",
       "      <td>Twizzlers, Strawberry my childhood favorite ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Home delivered twizlers</td>\n",
       "      <td>Candy was delivered very fast and was purchase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Always fresh</td>\n",
       "      <td>My husband is a Twizzlers addict.  We've bough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TWIZZLERS</td>\n",
       "      <td>I bought these for my husband who is currently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Delicious product!</td>\n",
       "      <td>I can remember buying this candy as a kid and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Twizzlers</td>\n",
       "      <td>I love this candy.  After weight watchers I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Please sell these in Mexico!!</td>\n",
       "      <td>I have lived out of the US for over 7 yrs now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Twizzlers - Strawberry</td>\n",
       "      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nasty No flavor</td>\n",
       "      <td>The candy is just red , No flavor . Just  plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Great Bargain for the Price</td>\n",
       "      <td>I was so glad Amazon carried these batteries. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>YUMMY!</td>\n",
       "      <td>I got this for my Mum who is not diabetic but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Best Hot Sauce in the World</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568381</th>\n",
       "      <td>Unusual, even for a violet candy</td>\n",
       "      <td>I've tried several violet flavored candies in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568382</th>\n",
       "      <td>Very good</td>\n",
       "      <td>This candy has a very good flavor. It is quite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568383</th>\n",
       "      <td>rip off</td>\n",
       "      <td>The candy is tasty, but they totally scam you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568384</th>\n",
       "      <td>The search has ended!</td>\n",
       "      <td>I had been looking for the violet candy with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568385</th>\n",
       "      <td>very tasty</td>\n",
       "      <td>these are very pricey so i only enjoy them now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568386</th>\n",
       "      <td>Violet French Hard Candy</td>\n",
       "      <td>These candies have a mild flavor, when compare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568387</th>\n",
       "      <td>delicious</td>\n",
       "      <td>This product is a bit pricey for the amt. rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568388</th>\n",
       "      <td>Mostly water</td>\n",
       "      <td>Definitely not worth buying flavored water wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568389</th>\n",
       "      <td>No Good</td>\n",
       "      <td>I thought this soup would be more like a chill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568390</th>\n",
       "      <td>Tastes horrible!</td>\n",
       "      <td>I just bought this soup today at my local groc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568391</th>\n",
       "      <td>Not so good</td>\n",
       "      <td>This soup is mostly broth. Although it has a k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568392</th>\n",
       "      <td>Where's the tortellini?</td>\n",
       "      <td>It is mostly broth, with the advertised 3/4 cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568393</th>\n",
       "      <td>Baker's Ammonia Small quantity</td>\n",
       "      <td>In the past, I would have to buy a large quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568394</th>\n",
       "      <td>Good buy.</td>\n",
       "      <td>Ammonium bicarbonate in a nice little package....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568395</th>\n",
       "      <td>Baking Ammonia that Works</td>\n",
       "      <td>If you haven't ever used Ammonium Bicarbonate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568396</th>\n",
       "      <td>a-ok</td>\n",
       "      <td>We need this for a recipe my wife is intereste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568397</th>\n",
       "      <td>Delicious, all natural and allergy free treats!</td>\n",
       "      <td>Indie Candy's gummies are absolutely delicious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568398</th>\n",
       "      <td>Great For Fast Gulasch!</td>\n",
       "      <td>Quick and easy! Had similar Gulasch in Guest H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568399</th>\n",
       "      <td>Great Cafe Latte</td>\n",
       "      <td>This product is great.  Gives you so much ener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568400</th>\n",
       "      <td>Excellent Tea</td>\n",
       "      <td>I love this tea.  I first discovered the pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568401</th>\n",
       "      <td>Best Value for Chinese 5 Spice</td>\n",
       "      <td>As a foodie, I use a lot of Chinese 5 Spice po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568402</th>\n",
       "      <td>Five Spice Powder</td>\n",
       "      <td>You can make this mix yourself, but the Star A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568403</th>\n",
       "      <td>Mixed wrong</td>\n",
       "      <td>I had ordered some of these a few months back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568404</th>\n",
       "      <td>If its all natural, this is like panacea of Sp...</td>\n",
       "      <td>Hoping there is no MSG in this, this tastes ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568405</th>\n",
       "      <td>Very large ground spice jars.</td>\n",
       "      <td>My only complaint is that there's so much of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568406</th>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568407</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568408</th>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568409</th>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568410</th>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568411 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Summary  \\\n",
       "0                                   Good Quality Dog Food   \n",
       "1                                       Not as Advertised   \n",
       "2                                   \"Delight\" says it all   \n",
       "3                                          Cough Medicine   \n",
       "4                                             Great taffy   \n",
       "5                                              Nice Taffy   \n",
       "6           Great!  Just as good as the expensive brands!   \n",
       "7                                  Wonderful, tasty taffy   \n",
       "8                                              Yay Barley   \n",
       "9                                        Healthy Dog Food   \n",
       "10                        The Best Hot Sauce in the World   \n",
       "11      My cats LOVE this \"diet\" food better than thei...   \n",
       "12                   My Cats Are Not Fans of the New Food   \n",
       "13                                      fresh and greasy!   \n",
       "14                           Strawberry Twizzlers - Yummy   \n",
       "15               Lots of twizzlers, just what you expect.   \n",
       "16                                             poor taste   \n",
       "17                                               Love it!   \n",
       "18                                     GREAT SWEET CANDY!   \n",
       "19                                Home delivered twizlers   \n",
       "20                                           Always fresh   \n",
       "21                                              TWIZZLERS   \n",
       "22                                     Delicious product!   \n",
       "23                                              Twizzlers   \n",
       "24                          Please sell these in Mexico!!   \n",
       "25                                 Twizzlers - Strawberry   \n",
       "26                                        Nasty No flavor   \n",
       "27                            Great Bargain for the Price   \n",
       "28                                                 YUMMY!   \n",
       "29                        The Best Hot Sauce in the World   \n",
       "...                                                   ...   \n",
       "568381                   Unusual, even for a violet candy   \n",
       "568382                                          Very good   \n",
       "568383                                            rip off   \n",
       "568384                              The search has ended!   \n",
       "568385                                         very tasty   \n",
       "568386                           Violet French Hard Candy   \n",
       "568387                                          delicious   \n",
       "568388                                       Mostly water   \n",
       "568389                                            No Good   \n",
       "568390                                   Tastes horrible!   \n",
       "568391                                        Not so good   \n",
       "568392                            Where's the tortellini?   \n",
       "568393                     Baker's Ammonia Small quantity   \n",
       "568394                                          Good buy.   \n",
       "568395                          Baking Ammonia that Works   \n",
       "568396                                               a-ok   \n",
       "568397    Delicious, all natural and allergy free treats!   \n",
       "568398                            Great For Fast Gulasch!   \n",
       "568399                                   Great Cafe Latte   \n",
       "568400                                      Excellent Tea   \n",
       "568401                     Best Value for Chinese 5 Spice   \n",
       "568402                                  Five Spice Powder   \n",
       "568403                                        Mixed wrong   \n",
       "568404  If its all natural, this is like panacea of Sp...   \n",
       "568405                      Very large ground spice jars.   \n",
       "568406                                Will not do without   \n",
       "568407                                       disappointed   \n",
       "568408                           Perfect for our maltipoo   \n",
       "568409                 Favorite Training and reward treat   \n",
       "568410                                        Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "5       I got a wild hair for taffy and ordered this f...  \n",
       "6       This saltwater taffy had great flavors and was...  \n",
       "7       This taffy is so good.  It is very soft and ch...  \n",
       "8       Right now I'm mostly just sprouting this so my...  \n",
       "9       This is a very healthy dog food. Good for thei...  \n",
       "10      I don't know if it's the cactus or the tequila...  \n",
       "11      One of my boys needed to lose some weight and ...  \n",
       "12      My cats have been happily eating Felidae Plati...  \n",
       "13      good flavor! these came securely packed... the...  \n",
       "14      The Strawberry Twizzlers are my guilty pleasur...  \n",
       "15      My daughter loves twizzlers and this shipment ...  \n",
       "16      I love eating them and they are good for watch...  \n",
       "17      I am very satisfied with my Twizzler purchase....  \n",
       "18      Twizzlers, Strawberry my childhood favorite ca...  \n",
       "19      Candy was delivered very fast and was purchase...  \n",
       "20      My husband is a Twizzlers addict.  We've bough...  \n",
       "21      I bought these for my husband who is currently...  \n",
       "22      I can remember buying this candy as a kid and ...  \n",
       "23      I love this candy.  After weight watchers I ha...  \n",
       "24      I have lived out of the US for over 7 yrs now,...  \n",
       "25      Product received is as advertised.<br /><br />...  \n",
       "26      The candy is just red , No flavor . Just  plan...  \n",
       "27      I was so glad Amazon carried these batteries. ...  \n",
       "28      I got this for my Mum who is not diabetic but ...  \n",
       "29      I don't know if it's the cactus or the tequila...  \n",
       "...                                                   ...  \n",
       "568381  I've tried several violet flavored candies in ...  \n",
       "568382  This candy has a very good flavor. It is quite...  \n",
       "568383  The candy is tasty, but they totally scam you ...  \n",
       "568384  I had been looking for the violet candy with t...  \n",
       "568385  these are very pricey so i only enjoy them now...  \n",
       "568386  These candies have a mild flavor, when compare...  \n",
       "568387  This product is a bit pricey for the amt. rece...  \n",
       "568388  Definitely not worth buying flavored water wit...  \n",
       "568389  I thought this soup would be more like a chill...  \n",
       "568390  I just bought this soup today at my local groc...  \n",
       "568391  This soup is mostly broth. Although it has a k...  \n",
       "568392  It is mostly broth, with the advertised 3/4 cu...  \n",
       "568393  In the past, I would have to buy a large quant...  \n",
       "568394  Ammonium bicarbonate in a nice little package....  \n",
       "568395  If you haven't ever used Ammonium Bicarbonate ...  \n",
       "568396  We need this for a recipe my wife is intereste...  \n",
       "568397  Indie Candy's gummies are absolutely delicious...  \n",
       "568398  Quick and easy! Had similar Gulasch in Guest H...  \n",
       "568399  This product is great.  Gives you so much ener...  \n",
       "568400  I love this tea.  I first discovered the pleas...  \n",
       "568401  As a foodie, I use a lot of Chinese 5 Spice po...  \n",
       "568402  You can make this mix yourself, but the Star A...  \n",
       "568403  I had ordered some of these a few months back ...  \n",
       "568404  Hoping there is no MSG in this, this tastes ex...  \n",
       "568405  My only complaint is that there's so much of i...  \n",
       "568406  Great for sesame chicken..this is a good if no...  \n",
       "568407  I'm disappointed with the flavor. The chocolat...  \n",
       "568408  These stars are small, so you can give 10-15 o...  \n",
       "568409  These are the BEST treats for training and rew...  \n",
       "568410  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568411 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the first n rows. n=5 as default\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews # 1\n",
      "Good Quality Dog Food\n",
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "\n",
      "Reviews # 2\n",
      "Not as Advertised\n",
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "\n",
      "Reviews # 3\n",
      "\"Delight\" says it all\n",
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "\n",
      "Reviews # 4\n",
      "Cough Medicine\n",
      "If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "\n",
      "Reviews # 5\n",
      "Great taffy\n",
      "Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Reviews #\", i+1)\n",
    "    print(reviews.Summary[i])\n",
    "    print(reviews.Text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Data\n",
    "- convert to lowercase\n",
    "- replace contractions with their longer forms. (contraction : 줄임말 등)\n",
    "- remove any unwanted characters (done after replacing contractions. backward slash before hyphen.)\n",
    "- remove stopwords from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# someone made contraction dictionary from wikipeda\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word]) # longer term을 nex_text에 append함.\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        # join() method takes all items in an iterable and joins them into one string.\n",
    "        text = \" \".join(new_text)        \n",
    "        \n",
    "        # Format words and remove unwanted characters\n",
    "        # ?는 0번 또는 1차례까지의 발생을 의미함. http 또는 https를 의미함.\n",
    "        # MULTILINE : '^'가 각 문자열, 문장의 처음에 매칭됨. '$'는 각 문자열과 문장의 마지막에 매칭됨.\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'\\&amp;', ' ', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        \n",
    "        # Optionally, remove stop words\n",
    "        if remove_stopwords:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the summaries and texts\n",
    "# stopwords will only be removed from the description to make training faster\n",
    "# but they will reamin in the summaries to make them sound more like natural phrases.\n",
    "\n",
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text, remove_stopwords=True))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "good quality dog food\n",
      "bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "\n",
      "Clean Review # 2\n",
      "not as advertised\n",
      "product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "\n",
      "Clean Review # 3\n",
      " delight  says it all\n",
      "confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "\n",
      "Clean Review # 4\n",
      "cough medicine\n",
      "looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "\n",
      "Clean Review # 5\n",
      "great taffy\n",
      "great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned all.\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\", i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    # build word histogram as dictioncary to count the word\n",
    "    \n",
    "    for sentence in text: # text가 하나의 문장을 element로 가진 배열\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 132884\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary both in summary and text\n",
    "# Summary와 Text에 나타나는 서로 다른 단어의 종류 개수.\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "\n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better\n",
    "<br />\n",
    "- use pre-trained word vectors to help improve the performance of our model.\n",
    "- **ConceptNet Numberbatch(CN)** : word embeddings that we are using.\n",
    "- **ConceptNet** : semantic network. 컴퓨터가 자연어의 의미를 이해하는데 도움이 되도록 만들어짐. https://github.com/commonsense/conceptnet-numberbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 418082\n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary\n",
    "embeddings_index = {}\n",
    "\n",
    "# f로 파일을 오픈함. f의 각 line에 대하여, line을 단어 별로 나누어 values에 할당. word는 value[0].\n",
    "# float32 type으로 values[1:]를 numpy array로 convert하여 embedding에 저장.\n",
    "# word가 key, embedding이 value.\n",
    "\n",
    "with open('/home/limhyesu/Summarization_study_data/numberbatch-en-17.04b.txt', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "        \n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 3866\n",
      "Percent of words that are missing from vocabulary: 2.91%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold\n",
    "# CN에 없고 threshold인 20번 보다 많이 등장한 단어의 개수를 찾는다.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "# Dict.items() returns dict_items object that connects key and value.\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "\n",
    "missing_ratio = round(missing_words/len(word_counts), 4)*100\n",
    "\n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CN에 없는 단어가 word_embedding_matrix에 더해지려면 그 단어는 적어도 20번 이상 등장해야 한다. 많이 등장해야 모델이 단어의 의미를 이해할 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Limit the vocab that we will use to words that appear >= threshold or arein Glove\n",
    "\n",
    "# dictionary to convert words to integers\n",
    "vocab_to_int = {}\n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index: # 20번 이상 등장하거나, CN에 있다면\n",
    "        vocab_to_int[word] = value # 단어마다 int 할당. \n",
    "        value += 1\n",
    "# vocab_to_int에는 CN에 있거나, CN에는 없지만 20번 이상 등장하는 단어가 key\n",
    "# 각 key에 대해 0부터 차례대로 int 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59604\n",
      "132884\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))\n",
    "print(len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sepcial tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<GO>\"]\n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int) # vocab_to_int의 마지막에 codes 추가.\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 132884\n",
      "Number of words we will use: 59608\n",
      "Percent of words we will use: 44.86%\n"
     ]
    }
   ],
   "source": [
    "usage_ratio = round(len(vocab_to_int)/ len(word_counts), 4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- limit vocabulary to words that are either **in CN** or **occur more than 20 times** in our dataset\n",
    "- model이 단어를 많이 볼 수록, 즉 단어가 많이 나타날 수록 단어들끼리 어떻게 연관되어 있는지 알기 쉽기 때문에 어휘를 위와 같이 제한하는 것이 좋은 word embedding을 만들 수 있다.\n",
    "- word_embedding_matrix를 만들 때 np.zeros의 dtype을 float32로 설정하는 것은 매우 중요하다. 초기값이 float64인데 이는 Tensorflow에서 안돌아가므로 32로 낮춰야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59608\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# np.zeros(shape, type, order) : return a new array of given shape and type, filled with zeros.\n",
    "# (nb_words, emabedding_dim) : shape of the new array.\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype = np.float32)\n",
    "\n",
    "# vocab_to_int : 사용할 단어, embeddings_index : CN에 있는 단어\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index: \n",
    "        # CN에 있는 word라면 embedding 그대로 추가\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # if word not in CN, create a random embedding for it \n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding # embeddings_index에 word와 embedding 추가함.\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "        \n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "        If word is not in vocab_to_int, use UNK's integer.\n",
    "        Total the number of words and UNKs.\n",
    "        Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentences_ints = []\n",
    "        for wrd in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentences_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentences_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentences_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentences_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 25679933\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "# summary를 int로 바꿈, text를 int로 바꾸고 eos 추가.\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos = True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count, 4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    # 각 sentence의 length를 counts 열에 작성.\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean        4.181624\n",
      "std         2.657872\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max        48.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean       41.996835\n",
      "std        42.520873\n",
      "min         1.000000\n",
      "25%        18.000000\n",
      "50%        29.000000\n",
      "75%        50.000000\n",
      "max      2085.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n",
      "115.0\n",
      "207.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90)) # compute the 90th percentile of the lengths_texts elements\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "9.0\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of the UNK appears in a sentence'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To help train the model faster, **sort** the reviews by the **length of the descriptions** form shortest to longest.\n",
    "- This maeks each batch to have descriptions of **similar lengths**, which will result int **less padding**, thus **less computing**.\n",
    "- Some reviews will not be included because of the number of UNK tokens in the description or summary. If there is more than 1 UNK in the description or any UNKs in the summary, the review will not be used. 의미있는 데이터로 모델을 만들고 싶기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452350\n",
      "452350\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to largest\n",
    "# Sorting is to make each batch to have descriptions of similar lengths, which will result in less padding, thus less computing.\n",
    "# Limit the length of summaries and texts based on the min and max ranges\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84 # 90% percentile\n",
    "max_summary_length = 13 # 99% percentile\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "# text에는 1개의 UNK 까지 허용. summary에는 UNK가 없어야 함.\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length):\n",
    "    for count, words in enumerate(int_summaries): # enumerate : 몇 번 째 반복문인지 확인 가능.\n",
    "        if(len(int_summaries[count]) >= min_length and\n",
    "           len(int_summaries[count]) <= max_summary_length and\n",
    "           len(int_texts[count]) >= min_length and\n",
    "           unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "           unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "           length == len(int_texts[count]) # min, max 사이의 범위에 있는 length가 text의 length일 때.\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for input to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # summary_length, text_length are the lengths of each sentence within a batch\n",
    "    # max_summary_length is maximum length of a summary within a batch\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    \n",
    "    # Computes the maximum of elements across dimensions of a tensor.\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='summary_length')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "    \n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    # ending = target_data의 마지막 단어를 추출함.\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    # dec_input = scalar value로 채워진 tensor를 만들어서 마지막에 <GO> 붙임.\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            # Cell 만들기\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                   input_keep_prob = keep_prob)\n",
    "            \n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                             initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw,\n",
    "                                                    input_keep_prob = keep_prob)\n",
    "            # Cell 구동\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                                    cell_bw, rnn_inputs, sequence_length,\n",
    "                                                                    dtype = tf.float32)\n",
    "            \n",
    "            # Join outputs since we are using a bidirectional RNN\n",
    "            enc_output = tf.concat(enc_output, 2)\n",
    "            \n",
    "            return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer,\n",
    "                           vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                       sequence_length=summary_length,\n",
    "                                                       time_major=False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                      training_helper,\n",
    "                                                      initial_state,\n",
    "                                                      output_layer)\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                          output_time_major=False,\n",
    "                                                          impute_finished=True,\n",
    "                                                          maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TrainingHelper** reads a sequence of integers from the encoding layer.\n",
    "<br />\n",
    "**BasicDecoder** processes the sequence with the decoding cell, an output layer, which is a fully connected layer. *initial_state* comes from our *DynamicAttentionWrapperState* that you will see soon.\n",
    "<br />\n",
    "**dynamic_decode** creates our outputs that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initail_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_toekns = tf.tile(tf.constant([start_token], dtype = tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                               start_tokens, end_token)\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state,\n",
    "                                                       output_layer)\n",
    "    \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**inference_decoding_layer** is very similar to training layer. The main difference is **GreedyEmbeddingHelper**, which uses the argmax of the output (treated as logits) and passes the result through an embedding layer to get the next input. Although it is asking for **start_tokens**, we only have one, < GO >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                  max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the trainig and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                          initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "            \n",
    "    output_layer = Dense(vocab_size, \n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev = 0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, text_length, normalize = False, name='BahdanauAttention')\n",
    "    \n",
    "    '''No DynamicAttentionWrapper in new version  so I changed the code'''\n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "#    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "    \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "                                                                   _zero_state_tensors(rnn_size,\n",
    "                                                                                      batch_size, tf.float32))\n",
    "    \n",
    "#    initial_state = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=enc_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state,\n",
    "                                                output_layer, vocab_size, max_summary_length)\n",
    "        \n",
    "        \n",
    "        #training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False,\n",
    "        #                                                         impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'],\n",
    "                                                 vocab_to_int['<EOS>'],\n",
    "                                                 dec_cell, initial_state, output_layer, max_summary_length,\n",
    "                                                 batch_size)\n",
    "        \n",
    "        #inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False,\n",
    "        #                                                         impute_finished=True, maximum_iterations=max_summary_length)\n",
    "        \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length,\n",
    "                 vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(dec_embed_input, embeddings, enc_output, \n",
    "                                                      enc_state, vocab_size, text_length, summary_length,\n",
    "                                                      max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size,\n",
    "                                                      num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    '''Pad sentences with <PAD> so that each sentence of batch has the same length'''\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    '''Batch summaries, texts, and the lengths of their sentences together'''\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i+batch_size]\n",
    "        texts_batch = texts[start_i:start_i+batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "            \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "            \n",
    "        yield pad_sentence_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limhyesu/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'DynamicAttentionWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-042320b1bf3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                      \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                      \u001b[0mmax_summary_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                      vocab_to_int, batch_size)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Create tensors for the training logits and inference logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-9c245b3790d9>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                       \u001b[0menc_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                       \u001b[0mmax_summary_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                       num_layers)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-0334e2e04f6f>\u001b[0m in \u001b[0;36mdecoding_layer\u001b[0;34m(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m'''No DynamicAttentionWrapper in new version  so I changed the code'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdec_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDynamicAttentionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mech\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'DynamicAttentionWrapper'"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs\n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                     targets, keep_prob, text_length, summary_length,\n",
    "                                                     max_summary_length, len(vocab_to_int)+1, rnn_size, num_layers,\n",
    "                                                     vocab_to_int, batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype = tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(trainig_logits, targets, masks)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "print(\"Graph is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "버전차이로 DynamicAttentionWrapper가 없어서 안돌아가는 중..!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "used subset of the data since the whole data will take too long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the data for training\n",
    "start = 200000\n",
    "end = start + 50000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\", len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0\n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0\n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\"\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(\"./\"+checkpoint+'.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run([train_op, cost], \n",
    "                               {input_data: texts_batch, targets: summaries_batch, lr: learning_rate,\n",
    "                                summary_length: summaries_lengths, text_length: texts_lengths,\n",
    "                                keep_prob: keep_probability})\n",
    "            \n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i >0:\n",
    "                print('Epoch {:>3}/{} Batch{:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                     .format(epoch_i, epochs, batch_i, len(sorted_texts_short) // batch_size,\n",
    "                            batch_loss / display_step, batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i >0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!')\n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, checkpoint)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "                \n",
    "            \n",
    "            # Reduce learnig rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            \n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Making Our Own Summaries\n",
    "\n",
    "To see the quality of the summaries that this model can generate, you cna either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value lik I have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    test = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "# input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                            # I think tat I will try a green apple next time.\"\n",
    "# text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0, len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    \n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')    \n",
    "    \n",
    "    # Multiply by batch_size to math the model's input parameters\n",
    "    answer_logits = sess.run(logtis, {input_data: [text]*batch_size,\n",
    "                                     summary_length: [np.random.randint(5,8)],\n",
    "                                     text_length: [len(text)]*batch_size,\n",
    "                                     keep_prob: 1.0})[0]\n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD\"]\n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words:  {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:    {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words:  {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
